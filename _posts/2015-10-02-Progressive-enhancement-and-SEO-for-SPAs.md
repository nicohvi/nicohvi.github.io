---
layout: post
title: SEO and progressive enhancement for SPAs
draft: false 
---

(For a TLDR of these principles, you can just check out the accompanying github repository[^1])

Consider the following: you've created a dashing single-page application sporting the recent trends in werewolf-themed sweaters (aptly named sweaterwolves.com).

You have a single view (`index.jade`) which renders the HTML, and then your HTTP server provides JSON endpoints which the SPA will consume through client-side javascript.

{% gist 8b73dcdb8d5cc2e3b258 %}

Your application basically has three pages: the home page containing information about the recent trends, an FAQ page (because there are always questions about these kinds of things), and of course an about page since you want to show the world your handsome face. 

![face](/public/images/posts/face.png)
*Smooth*

Once it's done you settle into your cosy chair and stroke your imaginary cat as you watch the visitors flood in using Google Analytics. But then you notice something disconcerting: hardly anyone visits your about page! 

This is horrible news, how else are you going to meet the werewolf-sweater loving woman of your dreams? You quickly open a new tab and google "werewolf sweaters about" and to your horror realise that the only result you find is a link to your main page.

You sink down to a fetal position on the floor and contemplate the cruelty of fate. Then, you being a determined individual, you decide to do something about it. 

So what do you do? You google it, of course. The problem with this approach is that there are many, *many* different approaches to this particular problem, and they all have various drawbacks and tricks. 

For instance, the approach suggested by Google (in their docs anyway) is to have static HTML-files generated in advance to be served to crawlers and other pages to be served to your regular users. How will it know which is which? Well, all you have to do is to map all fragment keys to their own escaped_fragment URL. 

Sound like a hassle? That's because it is. 

![google-seo](/public/images/posts/google-seo.png)
*Kill me*

Fortunately, there's a better way.

The basic concept behind updating a page with dynamic content is to intercept the click event on a link, send a call to the server based on the URL in the link's href attribute, and once the data has been sent to the client a callback function updates the DOM to reflect the change in state (for an example of this, see the `app.js` file in the gist).

Now this is usually done using a fragment URL which contains application state in parameters following a hash. The hash is important because when links with hashes are clicked they do not generate HTTP requests to the server because these hashes were traditionally used to indicate sections in a static HTML document (they map to CSS ids). 

The problem with this approach is that much like servers, crawlers ignore fragment URLs, and thus will never see the dynamic content. Additionally, we have no way of knowing how much javascript the crawler can execute, so we can't safely say whether or not it loads all our scripts, or if it can actually execute the code we want it to.

So, how do we fix this without jumping through too many hoops and ruin our day? The trick is to create actual views for the dynamic content you want to show up in search engine results as unique hits and then intercept the links to these views if the client is not a crawler and has javascript enabled. 

Holy progressive enhancement Batman, that sure sounds swell!

This way you can serve server-rendered views to crawlers (and users who have disabled javascript for whatever reason) and still load data asynchronously through event hijacking (like before). This is, more or less, progressive enhancement in action.

Creating views for all these pages might require you to create additional controller actions and view files, but for our simple example we can just add the different values for `title`, `body`, and `image` in our previously defined JSON endpoints, and render the `index` view with these new values if the request wasn't asynchronous.

Here's a very simple example from our lovely site sweaterwolves.com.

{% gist db28610c5f6fc5329223 %}

Once you have the views set up you can change the links in the page to direct to these views instead (so, in our example, `sweaterwolves.com/about` instead of `sweatertwolves.com#about`). 

Any user will now see the site as a crawler would, and the links will no longer fetch dynamic content, but will instead redirect you to the HTML generated by the server. 

<iframe src="https://player.vimeo.com/video/141150453" width="100%" height="380" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

As you can see HTTP requests are now being made to the server and the page is fully refreshed each time.

Congratulations, your website is now what it would have been like had you made it five years ago - nice and paginated! 

Don't worry, we won't leave things like this for long, I promise.

The good news is that since your URLs actually load server-generated HTML, you know that crawlers find and index your pages. On the negative side, you'd like your dynamic content to be, well, *dynamic*. 

To do this we once again intercept the click event on any links and then stop the event propagation using the famous `preventDefault()` method. 

Furthermore, we kindly ask our server to fetch the data for the dynamic content using the endpoit for the JSON data (which we used before we started all this). 

Once the data is fetched, we update the DOM and ta-dah! we're back to dynamic data.

{% gist 31bd7545831dde186ae4 app.js %}

Technically you could leave things like this and search engines would index and paginate the dynamic pages you want. However, it's a pretty bad idea to not update the address bar when one of your users clicks a link in your website. 

It's pretty much the most basic functionality expected from a website, so not offering it lowers the chance for hooking up with sweater-loving honeys dramatically. Seriously.

Enter the [window.history API](https://developer.mozilla.org/en-US/docs/Web/API/History_API), introduced in HTML5. 

This API offers methods to manipulate the history session object, which in turn decides what shows up in the user's address bar. By using the `history.pushState()` method we can push anything we want onto the history stack, and so we can add the URL from the DOM element which our event was invoked upon. 

Now we have a working address bar again, huzzah!

{% gist 31bd7545831dde186ae4 app-address.js %}

"But wait!" you rudely interrupt. "What about the back button? How will the page show the correct data?" Easy. You just attach a handler to the [onpopstate](https://developer.mozilla.org/en-US/docs/Web/API/WindowEventHandlers/onpopstate) event, and replace the HTML in the DOM based on the new URL.

Something like this:

{% gist 31bd7545831dde186ae4 back-button.js %}

And just like that you have all the sexy fluidity and dynamic prowess of AJAX and your pages will show up as unique hits in search engines.And, even more importantly some (like me) would argue, your website now supports [progressive enhancement](http://alistapart.com/article/understandingprogressiveenhancement).

The theme-sweater loving girl of your dreams is only a search query away.

---
[^1]: Found [here](https://github.com/nicohvi/spa-seo).
