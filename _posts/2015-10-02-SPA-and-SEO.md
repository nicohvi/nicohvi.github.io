---
layout: post
title: SPA and SEO
draft: true
---

(For a TLDR of these principles, you can just check out the accompanying github repository[^1])

Consider the following: you've created a dashing single-page application sporting the recent trends in werewolf-themed sweaters (aptly named sweaterwolves.com). Since javascript is the bomb right now, you decide that you want to create a snappy single-page application (let's just call them SPAs from now on, because it's a long flippin' word).

In essence all you need then is a single index view which will render the HTML you want, and then your application can provide endpoints which will yield JSON consumed by your SPA. Easy peasy!

{% gist 8b73dcdb8d5cc2e3b258 %}

Your application basically has three pages: the main page containing links to articles about recent trends, an FAQ page (because there are always questions about these kinds of things), and of course an about page since you want to show the world your handsome face. 

![face](/public/images/posts/face.png)
*Smooth*

Once it's done you settle into your cosy chair and stroke your imaginary cat as you watch the visitors flood in using Google Analytics. But then you notice something disconcerting: hardly anyone visits your about page! 

This is horrible news, how else are you going to meet the werewolf-sweater loving woman of your dreams? You quickly open a new tab and google "werewolf sweaters about" and to your horror realise that the only result that google shows is a link to your main site. 

You sink down to a fetal position on the floor and contemplate the cruelty of fate. Then, you being a determined individual, you decide to do something about it. 

So what do you do? You google it, of course. The problem with this approach is that there are many, *many* different approaches to this particular problem, and they all have with various drawbacks and tricks. For instance, the approach suggested by google (in their docs anyway) is to have static HTML-files generated in advance to be served to crawlers and other pages to be served to your regular users. How will it know which is which? Well, all you have to do is map all various fragment keys to their own escaped_fragment URL. Sound like a hassle? That's because it is. 

![google-seo](/public/images/posts/google-seo.png)
*Kill me*

Fortunately, there's a better way.

The basic concept behind updating a page with dynamic content is to intercept the click event on a link, send a call to the server based on the URL in the link's href attribute, and once the data has been sent to the client a callback function updates the DOM to reflect the change in state (for an example of this, see the `app.js` file in the gist).

Now this is usually done using a fragment URL which contains application state in parameters following a hash. The hash is important because when links with hashes are clicked they do not generate HTTP requests to the server because these hashes were traditionally used to indicate sections in a static HTML document (they map to CSS ids). 

The problem with this approach is that much like servers, crawlers ignore fragment URLs, and thus google's crawler will never see the dynamic content. Additionally, we have no way of knowing how much javascrit can be executed by the crawler, so we can't safely say whether or not it loads all our scripts, or it can actually execute the code we want it to.

So, how do we fix this without jumping through too many hoops and ruin our day? The trick is to create actual views for the dynamic content you want to show up in google as unique hits and then intercept the links to these views if the client is not a crawler and has javascript enabled. 

This way you can serve these static HTML documents to crawlers (and users who have disabled javascript for whatever reason) and still load data asynchronously through event hijacking (like before). So creating a new view for the pages should be a walk in the park, and then you just wire it up to a controller to respond with the correct view. This could preferably be the same controller (and action) that would've otherwise responded with JSON (or HTML for that matter) in the original dynamic page.

Here's a very simple example from our lovely site sweaterwolves.com.

{% gist db28610c5f6fc5329223 %}

In an actual use case you would of course use a common layout template and the same view for all your pages. But I'm lazy.

Once you have the views set up you can change the links in the page to direct to these views instead (so, in our example, swetwolves.com/about instead of swetwolves.com#about). 

Any user will now see the site as a crawler would, and the links will no longer fetch dynamic content, but will instead redirect you to the HTML generated by the new views. 

<iframe src="https://player.vimeo.com/video/141150453" width="100%" height="380" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

As you can see HTTP requests are now being made to the server and the page is fully refreshed each time.

Congratulations, your website is now what it would have been like had you made it five years ago! Nice and paginated. Don't worry, we won't leave things like this for long, I promise.

The good news is that since your URLs work you know that webcrawlers can crawl and find all your pages. On the other hand, what you really want is to have the dynamic content still be, well, **dynamic**. 

To do this we once again intercept the click event on any links and then stop the event propagation using the famous `preventDefault()` method. 

Furthermore, we kindly ask our server to fetch the data for the dynamic content using the endpoit for the JSON data (which we used before we started all this). Once the data is fetched, we update the DOM and ta-dah! we're back to dynamic data.

{% gist 31bd7545831dde186ae4 app.js %}

Technically you could leave things like this and google would index and paginate the dynamic pages you want, but it's a pretty bad idea to not update the address bar when one of your users clicks a link in your website. It's pretty much the most basic functionality expected from a website, so not offering it lowers the chance for hooking up with sweater-loving honeys dramatically. Seriously.

Enter the [window.history API](https://developer.mozilla.org/en-US/docs/Web/API/History_API), introduced in HTML5. 

This API offers methods to manipulate the history session object, which in turn is what shows up in the user's address bar. By using the history.pushState() method we can push anything we want onto the history stack, and so we can add the URL from the DOM element which our event was invoked upon. 

Now we have a working address bar again, huzzah!

{% gist 31bd7545831dde186ae4 app-address.js %}

"But wait!" you rudely interrupt. "What about the back button? How will the page show the correct data?" Easy. You just attach a handler to the onpopstate event, and replace the data in the DOM based on the URL (hopefully you've cached this data already).

You have now successfully added each of your pages to google searches since the crawler can index and traverse them. There is one final problem.

When a user googles "werewolf sweaters about", finds the result for sweaterwolves.com/about, and clicks the link she'll be taken to the newly created `about` view. But what if we want to only show this static view to crawlers and instead want all our human users to interact with the javascript loaded in the `index` view?

The solution to this problem is to check in your controller whether or not the request has been made by a crawler (this can be deduced using the user agent string), and if that is the case then you redirect to the appropriate controller/action and set a flag so the view knows which data it should load initially.

Here's a quick and dirty example of how this can be done.

{% gist 31bd7545831dde186ae4 index.js %}

And now we just store in the session which article was requested
by the user (which is not a crawler) and pass this information along to the client-side javascript.

{% gist 31bd7545831dde186ae4 final.js %}

And just like that you have all the sexy fluidity and dynamic prowess of AJAX and your pages will show up as unique hits in google searches. The theme-sweater loving girl of your dreams is only a search query away.

---
[^1]: Found [here](https://github.com/nicohvi/spa-seo).
